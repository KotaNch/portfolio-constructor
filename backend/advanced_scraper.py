#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π —Å–∫—Ä—ç–π–ø–µ—Ä –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –≤—Å–µ—Ö –ø–æ—Ä—Ç—Ñ–æ–ª–∏–æ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç Selenium –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ JavaScript-rendered –∫–æ–Ω—Ç–µ–Ω—Ç–∞
"""

import requests
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import json
import os
import time
from typing import List, Dict
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class AdvancedFTCPortfolioScraper:
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π —Å–∫—Ä—ç–π–ø–µ—Ä –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤—Å–µ—Ö –ø–æ—Ä—Ç—Ñ–æ–ª–∏–æ FTC"""
    
    def __init__(self):
        self.base_url = "https://portfolios.hivemindrobotics.net/ftc"
        self.cdn_url = "https://cdn.hivemindrobotics.net"
        self.portfolios = []
        
    def scrape_with_selenium(self) -> List[Dict]:
        """–°–∫—Ä—ç–π–ø–∏—Ç —Å–∞–π—Ç –∏—Å–ø–æ–ª—å–∑—É—è Selenium (–¥–ª—è JavaScript –∫–æ–Ω—Ç–µ–Ω—Ç–∞)"""
        logger.info("–ó–∞–ø—É—Å–∫ Selenium –¥–ª—è —Å–∫—Ä—ç–π–ø–∏–Ω–≥–∞...")
        
        options = webdriver.ChromeOptions()
        options.add_argument('--start-maximized')
        options.add_argument('--disable-notifications')
        
        try:
            driver = webdriver.Chrome(options=options)
            driver.get(self.base_url)
            
            # –ñ–¥—ë–º –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            time.sleep(3)
            
            # –°–∫—Ä–æ–ª–∏–º –≤–Ω–∏–∑ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –≤—Å–µ—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
            last_height = driver.execute_script("return document.body.scrollHeight")
            while True:
                driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(2)
                new_height = driver.execute_script("return document.body.scrollHeight")
                if new_height == last_height:
                    break
                last_height = new_height
            
            logger.info("–ö–æ–Ω—Ç–µ–Ω—Ç –∑–∞–≥—Ä—É–∂–µ–Ω, –ø–∞—Ä—Å–∏–º...")
            
            # –ò—â–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ –ø–æ—Ä—Ç—Ñ–æ–ª–∏–æ
            pdf_links = driver.find_elements(By.XPATH, "//a[contains(@href, '/portfolios/')]")
            logger.info(f"–ù–∞–π–¥–µ–Ω–æ —Å—Å—ã–ª–æ–∫ –Ω–∞ PDF: {len(pdf_links)}")
            
            portfolios_dict = {}
            
            for link in pdf_links:
                try:
                    href = link.get_attribute('href')
                    if not href or '/portfolios/' not in href:
                        continue
                    
                    if not href.startswith('http'):
                        href = self.cdn_url + href
                    
                    # –ü–∞—Ä—Å–∏–º URL
                    filename = href.split('/portfolios/')[-1].replace('.pdf', '')
                    parts = filename.split('-')
                    
                    if len(parts) >= 2:
                        team_number = parts[0]
                        portfolio_type = '-'.join(parts[1:])
                        
                        # –ò—â–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é —Ä—è–¥–æ–º —Å —Å—Å—ã–ª–∫–æ–π
                        parent = link.find_element(By.XPATH, "..")
                        
                        # –ò—â–µ–º —Ç–µ–∫—Å—Ç —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –∫–æ–º–∞–Ω–¥–µ
                        try:
                            heading = parent.find_element(By.XPATH, ".//h1 | .//h2 | .//h3")
                            text = heading.text
                            if '‚Ä¢' in text:
                                parts_text = text.split('‚Ä¢')
                                team_name = parts_text[0].strip()
                                achievement = parts_text[1].strip() if len(parts_text) > 1 else "N/A"
                            else:
                                team_name = text
                                achievement = "N/A"
                        except:
                            team_name = "Unknown"
                            achievement = "N/A"
                        
                        key = f"{team_number}-{portfolio_type}"
                        if key not in portfolios_dict:
                            portfolios_dict[key] = {
                                'team_number': team_number,
                                'team_name': team_name,
                                'achievement': achievement,
                                'portfolio_type': portfolio_type,
                                'pdf_url': href,
                                'thumbnail_url': href.replace('/portfolios/', '/thumbnails/'),
                                'page_url': self.base_url
                            }
                except Exception as e:
                    logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ —ç–ª–µ–º–µ–Ω—Ç–∞: {e}")
                    continue
            
            self.portfolios = list(portfolios_dict.values())
            driver.quit()
            
            return self.portfolios
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ Selenium: {e}")
            logger.info("–ü–æ–ø—ã—Ç–∞—é—Å—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å fallback –º–µ—Ç–æ–¥...")
            return self.scrape_fallback()
    
    def scrape_fallback(self) -> List[Dict]:
        """–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥ –µ—Å–ª–∏ Selenium –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç"""
        logger.info("–ò—Å–ø–æ–ª—å–∑—É—é fallback –º–µ—Ç–æ–¥ (BeautifulSoup)...")
        
        try:
            response = requests.get(self.base_url, timeout=10)
            response.encoding = 'utf-8'
            
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(response.content, 'html.parser')
            
            portfolios_dict = {}
            
            # –ò—â–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ PDF
            pdf_links = soup.find_all('a', {'href': lambda x: x and '/portfolios/' in x})
            logger.info(f"–ù–∞–π–¥–µ–Ω–æ —Å—Å—ã–ª–æ–∫: {len(pdf_links)}")
            
            for link in pdf_links:
                try:
                    href = link.get('href', '')
                    if not href.startswith('http'):
                        href = self.cdn_url + href
                    
                    filename = href.split('/portfolios/')[-1].replace('.pdf', '')
                    parts = filename.split('-')
                    
                    if len(parts) >= 2:
                        team_number = parts[0]
                        portfolio_type = '-'.join(parts[1:])
                        
                        # –ò—â–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–º —ç–ª–µ–º–µ–Ω—Ç–µ
                        parent = link.parent
                        team_name = "Unknown"
                        achievement = "N/A"
                        
                        if parent:
                            text = parent.get_text(strip=True)
                            if '‚Ä¢' in text:
                                parts_text = text.split('‚Ä¢')
                                team_name = parts_text[0].strip()
                                achievement = parts_text[1].strip() if len(parts_text) > 1 else "N/A"
                        
                        key = f"{team_number}-{portfolio_type}"
                        if key not in portfolios_dict:
                            portfolios_dict[key] = {
                                'team_number': team_number,
                                'team_name': team_name,
                                'achievement': achievement,
                                'portfolio_type': portfolio_type,
                                'pdf_url': href,
                                'thumbnail_url': href.replace('/portfolios/', '/thumbnails/')
                            }
                except Exception as e:
                    logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ: {e}")
                    continue
            
            self.portfolios = list(portfolios_dict.values())
            return self.portfolios
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ fallback: {e}")
            return []
    
    def download_portfolio_pdfs(self, output_dir: str = 'downloaded_portfolios') -> int:
        """–°–∫–∞—á–∏–≤–∞–µ—Ç –≤—Å–µ –ø–æ—Ä—Ç—Ñ–æ–ª–∏–æ PDF —Ñ–∞–π–ª—ã"""
        os.makedirs(output_dir, exist_ok=True)
        
        downloaded = 0
        for i, portfolio in enumerate(self.portfolios, 1):
            try:
                url = portfolio['pdf_url']
                filename = f"Team_{portfolio['team_number']}_{portfolio['portfolio_type']}.pdf"
                filepath = os.path.join(output_dir, filename)
                
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –µ—Å–ª–∏ —É–∂–µ —Å–∫–∞—á–∞–Ω–æ
                if os.path.exists(filepath):
                    logger.info(f"{i}/{len(self.portfolios)} ‚úì {filename} (—É–∂–µ –µ—Å—Ç—å)")
                    downloaded += 1
                    continue
                
                response = requests.get(url, timeout=10)
                if response.status_code == 200:
                    with open(filepath, 'wb') as f:
                        f.write(response.content)
                    logger.info(f"{i}/{len(self.portfolios)} ‚úì {filename}")
                    downloaded += 1
                else:
                    logger.warning(f"{i}/{len(self.portfolios)} ‚úó {filename} (—Å—Ç–∞—Ç—É—Å {response.status_code})")
                
                time.sleep(0.5)  # –ù–µ –ø–µ—Ä–µ–≥—Ä—É–∂–∞–µ–º —Å–µ—Ä–≤–µ—Ä
                
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ {portfolio['team_number']}: {e}")
                continue
        
        return downloaded
    
    def save_metadata(self, filename: str = 'ftc_portfolios_full.json'):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –≤—Å–µ—Ö –ø–æ—Ä—Ç—Ñ–æ–ª–∏–æ"""
        output_path = os.path.join(os.path.dirname(__file__), 'data', filename)
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump({
                'portfolios': self.portfolios,
                'total_count': len(self.portfolios),
                'timestamp': str(__import__('datetime').datetime.now())
            }, f, ensure_ascii=False, indent=2)
        
        logger.info(f"–ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {output_path}")
        return output_path

if __name__ == '__main__':
    scraper = AdvancedFTCPortfolioScraper()
    
    print("=" * 60)
    print("üöÄ –ü–†–û–î–í–ò–ù–£–¢–´–ô –°–ö–†–≠–ô–ü–ï–† –ü–û–†–¢–§–û–õ–ò–û FTC")
    print("=" * 60)
    print()
    
    # 1. –°–∫—Ä—ç–π–ø–∏–º —Å–∞–π—Ç
    print("1Ô∏è‚É£  –°–∫—Ä—ç–π–ø–∏–º —Å–∞–π—Ç...")
    portfolios = scraper.scrape_with_selenium()
    print(f"‚úì –ù–∞–π–¥–µ–Ω–æ –ø–æ—Ä—Ç—Ñ–æ–ª–∏–æ: {len(portfolios)}")
    
    if len(portfolios) > 0:
        print("\n–ü—Ä–∏–º–µ—Ä—ã –ø–æ—Ä—Ç—Ñ–æ–ª–∏–æ:")
        for p in portfolios[:5]:
            print(f"  - Team {p['team_number']}: {p['team_name']} ({p['achievement']})")
        
        # 2. –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        print("\n2Ô∏è‚É£  –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ...")
        scraper.save_metadata()
        print("‚úì –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã")
        
        # 3. –°–∫–∞—á–∏–≤–∞–µ–º PDF
        print("\n3Ô∏è‚É£  –°–∫–∞—á–∏–≤–∞–µ–º PDF –ø–æ—Ä—Ç—Ñ–æ–ª–∏–æ...")
        print("(—ç—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –≤—Ä–µ–º—è, —Ç–∞–∫ –∫–∞–∫ –Ω—É–∂–Ω–æ —Å–∫–∞—á–∞—Ç—å –º–Ω–æ–≥–æ —Ñ–∞–π–ª–æ–≤)")
        downloaded = scraper.download_portfolio_pdfs()
        print(f"‚úì –°–∫–∞—á–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: {downloaded}/{len(portfolios)}")
    
    print("\n" + "=" * 60)
    print(f"‚úÖ –ì–û–¢–û–í–û! –í—Å–µ–≥–æ –ø–æ—Ä—Ç—Ñ–æ–ª–∏–æ: {len(portfolios)}")
    print("=" * 60)
